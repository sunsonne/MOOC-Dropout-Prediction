{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from random import random\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import re\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = pyspark.SparkConf().set('spark.executor.memory', '8G').set('spark.cores.max', '32')\\\n",
    "            .set('spark.port.maxRetries','200').set(\"spark.driver.maxResultSize\", \"3g\")\n",
    "sc = pyspark.SparkContext(appName=\"Project #2\", master='spark://polyp1:7077',conf = config )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = sc.textFile('activity_log.csv')\n",
    "train_label = sc.textFile('train_label.csv')\n",
    "enrollment_id = sc.textFile('enrollment_list.csv')\n",
    "sample_submission = sc.textFile('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'1', datetime.datetime(2014, 5, 31, 12, 43, 20), u'navigate']] 8157277\n"
     ]
    }
   ],
   "source": [
    "# split the input data and filter out first line\n",
    "# convert the datetime type\n",
    "\n",
    "data_raw = input_data.filter(lambda e: e[0] != 'e').map(lambda e: e.split(',')).map(\n",
    "    lambda e: [e[0], datetime.datetime.strptime(e[1], \"%Y-%m-%dT%H:%M:%S\"), e[2] ] )\n",
    "                                                                                \n",
    "print data_raw.take(1), data_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'1', 1.0)]\n",
      "[[u'1', u'Mv7P1v8fRDifebDlgIi2Vl1Z', u'ev4oSPQOjeL5QAf5oF72ooYu']]\n",
      "[[u'72326', u'0.89875500179']]\n"
     ]
    }
   ],
   "source": [
    "# change the format of other files\n",
    "\n",
    "train_data = train_label.filter(lambda e: e[0] != 'e').map(lambda e: e.split(',')).map(lambda e: (e[0], float(e[1])) )\n",
    "enrollment_data = enrollment_id.filter(lambda e: e[0] != 'e').map(lambda e: e.split(','))\n",
    "submission_data = sample_submission.filter(lambda e: e[0] != 'e').map(lambda e: e.split(','))\n",
    "\n",
    "print train_data.take(1)\n",
    "print enrollment_data.take(1)\n",
    "print submission_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'1', datetime.datetime(2014, 5, 31, 12, 43, 20), 'C']]\n"
     ]
    }
   ],
   "source": [
    "# convert all activities to a dictionary value\n",
    "# check if there are only seven kinds of value in activity.\n",
    "\n",
    "dic_activity = {'access':'A', 'discussion':'B', 'navigate':'C', 'page_close':'D', \n",
    "                'problem':'E', 'video':'F', 'wiki':'G' }\n",
    "dic_list = ['A','B','C','D','E','F','G']\n",
    "\n",
    "data_type = data_raw.map(lambda e: [ e[0], e[1], dic_activity[e[2]] ] )\n",
    "print data_type.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are only 7 activities --> it is clean, and can apply dictionary to map them   \n",
    "### Some id contains records of different days, 3944 in total, should be carefully designed for these data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'110557', [311, 55, 57, 124, 62, 3, 3]]] 120542\n",
      "['access', 'discussion', 'navigate', 'page_close', 'problem', 'video', 'wiki']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" create the frequency for each activity in each enrollment_id --> 7 features created \"\"\"\n",
    "# the head names are in same order of the values\n",
    "\n",
    "def freq_id(e):\n",
    "    dic_freq = {'A':0, 'B':0, 'C':0, 'D':0, 'E':0, 'F':0, 'G':0}\n",
    "    key_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "    value_list = []\n",
    "    for x in e[1]:\n",
    "        dic_freq[x] += 1\n",
    "    for i in range(7):\n",
    "        value_list.append(dic_freq[key_list[i]])\n",
    "    return [e[0], value_list]\n",
    "\n",
    "data_freq = data_type.map(lambda e: [e[0], e[2]]).groupByKey().map(lambda e: [ e[0], [x for x in e[1]] ] ).map(freq_id)\n",
    "data_freq_head = sorted(dic_activity.keys())\n",
    "\n",
    "print data_freq.take(1), data_freq.count()\n",
    "print data_freq_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'110557', 8)] 120542\n"
     ]
    }
   ],
   "source": [
    "\"\"\"how many days in an enrollment_id --> 1 feature created\"\"\"\n",
    "\n",
    "data_day = data_type.map(\n",
    "    lambda e: [(e[0] + ',' + str(e[1].year) + ',' + str(e[1].timetuple().tm_yday)), 1 ] ).groupByKey().map(\n",
    "    lambda e: e[0] ).map(lambda e: e.split(',')).map(lambda e: (e[0], 1) ).reduceByKey(lambda x,y: x+y)\n",
    "data_day_head = ['number of days']\n",
    "\n",
    "print data_day.take(1), data_day.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'110557', 15508.25]] 120542\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Time lag in a day in seconds, for those who have multiple day records, take average  --> 1 feature created \"\"\"\n",
    "\n",
    "data_timeLag = data_type.map(\n",
    "    lambda e: [(e[0] + ',' + str(e[1].year) + ',' + str(e[1].timetuple().tm_yday)), e[1] ] ).groupByKey().map(\n",
    "    lambda e: [ e[0], [x for x in e[1]] ] ).map(\n",
    "    lambda e: [ e[0], (max(e[1]) - min(e[1])).total_seconds() ] ).map(\n",
    "    lambda e: [ e[0].split(','), e[1]] ).map(\n",
    "    lambda e: [ e[0][0], e[1]] ).groupByKey().map(\n",
    "    lambda e: [ e[0], np.mean([x for x in e[1]]) ] )\n",
    "data_timeLag_head = ['working time interval']\n",
    "\n",
    "print data_timeLag.take(1), data_timeLag.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a little harder feature generation --> activity pair pattern related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'D'), (0, u'A')]\n",
      "[(0, [u'E', u'D', u'D', u'A']), (1, [u'E']), (2, [u'F'])]\n"
     ]
    }
   ],
   "source": [
    "# Make sure that 'groupByKey' method can keep the pattern order\n",
    "\n",
    "example = sc.parallelize([(0, u'D'), (0, u'A'), (0, u'E'), (0, u'D'),(1, u'E'), (2, u'F')])\n",
    "a = example.groupByKey().map(lambda x : (x[0], list(x[1]))).collect()\n",
    "print example.take(2)\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'110557', [9362, 1014, 1456, 1398, 1461, 3830, 3649, 2499, 1098, 134, 58, 54, 105, 338, 462, 332, 765, 436, 266, 154, 7, 8, 40, 29, 463, 765, 633, 269, 199, 6, 7, 44, 43, 1493, 937, 418, 44, 20, 32, 64, 391, 28, 27, 7, 14, 1, 0, 0, 3]]] 120542\n",
      "['AA', 'AB', 'BA', 'AC', 'CA', 'AD', 'DA', 'AE', 'EA', 'AF', 'FA', 'AG', 'GA', 'BB', 'BC', 'CB', 'BD', 'DB', 'BE', 'EB', 'BF', 'FB', 'BG', 'GB', 'CC', 'CD', 'DC', 'CE', 'EC', 'CF', 'FC', 'CG', 'GC', 'DD', 'DE', 'ED', 'DF', 'FD', 'DG', 'GD', 'EE', 'EF', 'FE', 'EG', 'GE', 'FF', 'FG', 'GF', 'GG']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pair pattern: Frequency for each two-activity pattern --> 49 features created\"\"\"\n",
    "\n",
    "# create a pair-pattern dictionary for future mapping\n",
    "\n",
    "list_act = ['A','B','C','D','E','F','G']\n",
    "list_value = np.zeros(49)\n",
    "list_key = []\n",
    "\n",
    "for i in xrange(7):\n",
    "    list_key.append(list_act[i] + list_act[i])\n",
    "    for j in xrange(i+1,7):\n",
    "        list_key.append(list_act[i] + list_act[j])\n",
    "        list_key.append(list_act[j] + list_act[i])\n",
    "        \n",
    "dic_pair_pattern = dict(zip(list_key, list(list_value)))  # create pattern dictionary outside to maintain the key order\n",
    "\n",
    "def pair_pattern(e):\n",
    "    for k in dic_pair_pattern.keys():   # initialize dictionary\n",
    "        dic_pair_pattern[k] = 0\n",
    "    for i in xrange(len(e[1])):         # calculation\n",
    "        lab = e[1][i]\n",
    "        for j in xrange(i+1,len(e[1])):\n",
    "            label = lab + e[1][j]\n",
    "            dic_pair_pattern[label] += 1\n",
    "    value_list = []                     # create a value list to make sure that output is in same order as column names\n",
    "    for m in xrange(49):\n",
    "        value_list.append(dic_pair_pattern[list_key[m]])\n",
    "    return [ e[0], value_list ] \n",
    "\n",
    "def pair_pattern_merge(e):              # add value from different day operations but belongs to same enrollment_id\n",
    "    list_pair_pattern = []\n",
    "    for i in xrange(49):\n",
    "        freq = 0\n",
    "        for value in e[1]:\n",
    "            freq += value[i]\n",
    "        list_pair_pattern.append(freq)\n",
    "    return [ e[0], list_pair_pattern ]\n",
    "            \n",
    "data_pairPattern = data_type.map(\n",
    "    lambda e: [(e[0] + ',' + str(e[1].year) + ',' + str(e[1].timetuple().tm_yday)), e[2] ] ).groupByKey().map(\n",
    "    lambda e: [ e[0], [x for x in e[1]] ] ).map(pair_pattern).map(\n",
    "    lambda e: [ e[0].split(','), e[1]] ).map(\n",
    "    lambda e: [ e[0][0], e[1]] ).groupByKey().map(\n",
    "    lambda e: [ e[0], [x for x in e[1]] ] ).map(pair_pattern_merge)\n",
    "\n",
    "data_pairPattern_head = list_key        # create a list as column head to understand the new feature\n",
    "\n",
    "print data_pairPattern.take(1), data_pairPattern.count()\n",
    "print data_pairPattern_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'110557', [85873.0, 3293.0, 5991.0, 18715.0, 10053.0, 2.0, 139.0]]] 120542\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Activity lag: how long an activity last in seconds --> 7 features \"\"\"\n",
    "\n",
    "# need to think about the 'page_close' activity --> maybe it does not make sense\n",
    "# create a activity dictionary for future mapping\n",
    "\n",
    "activity_key = ['A','B','C','D','E','F','G']\n",
    "activity_value = np.zeros(7)\n",
    "activity_lag = dict( zip(activity_key, list(activity_value)) )\n",
    "\n",
    "def activity_time_lag(e):\n",
    "    for key in activity_key:            # initialize dictionary\n",
    "        activity_lag[key] = 0\n",
    "    for i in xrange(0,len(e[1])-1):     # add time lag for each activity\n",
    "        label = e[1][i][0]\n",
    "        value = (e[1][i+1][1] - e[1][i][1]).total_seconds()\n",
    "        activity_lag[label] += value\n",
    "    alist = []\n",
    "    for j in xrange(7):                 # created ordered time lag values for each activity\n",
    "        alist.append(activity_lag[activity_key[j]])\n",
    "    return [ e[0], alist ]\n",
    "\n",
    "def activity_lag_merge(e):              # add value from different day operations but belongs to same enrollment_id\n",
    "    act_time_lag = []\n",
    "    for i in xrange(7):\n",
    "        lag = 0\n",
    "        for value in e[1]:\n",
    "            lag += value[i]\n",
    "        act_time_lag.append(lag)\n",
    "    return [ e[0], act_time_lag ]\n",
    "\n",
    "data_activity_lag = data_type.map(\n",
    "    lambda e: [(e[0] + ',' + str(e[1].year) + ',' + str(e[1].timetuple().tm_yday)), (e[2], e[1])] ).groupByKey().map(\n",
    "    lambda e: [ e[0], [x for x in e[1]] ] ).map(activity_time_lag).map(\n",
    "    lambda e: [ e[0].split(','), e[1]] ).map(\n",
    "    lambda e: [ e[0][0], e[1]] ).groupByKey().map(\n",
    "    lambda e: [ e[0], [x for x in e[1]] ] ).map(activity_lag_merge)\n",
    "\n",
    "data_activity_lag_head = activity_key        # create a list as column head to understand the new feature\n",
    "\n",
    "print data_activity_lag.take(1), data_activity_lag.count()\n",
    "print data_activity_lag_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join new features and train label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### features need to be joined\n",
    "\n",
    "data_day, data_day_head --> 1\n",
    "data_timeLag, data_timeLag_head --> 1\n",
    "data_freq, data_freq_head --> 7\n",
    "\n",
    "data_pairPattern, data_pairPattern_head --> 49\n",
    "data_activity_lag, data_activity_lag_head --> 7\n",
    "\n",
    "#### course is used as an id for later calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'74702', [4, 1218.25, 23, 0, 7, 14, 0, 5, 0, 90, 0, 0, 20, 42, 88, 56, 0, 0, 30, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 27, 17, 0, 0, 10, 10, 0, 0, 46, 0, 0, 20, 30, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 53.0, 0, 86.0, 4734.0, 0, 0.0, 0])] 120542 65\n"
     ]
    }
   ],
   "source": [
    "# new rdd file with enrollment_id and all features\n",
    "\n",
    "def break_list(e):\n",
    "    alist = []\n",
    "    for i in range(2):\n",
    "        for j in xrange(len(e[1][i])):\n",
    "            alist.append(e[1][i][j])\n",
    "    return ( e[0], alist )\n",
    "\n",
    "data_features = data_day.join(data_timeLag).join(data_freq).map(break_list).join(data_pairPattern).map(\n",
    "    break_list).join(data_activity_lag).map(break_list)\n",
    "\n",
    "feature_list = data_features.take(1)\n",
    "number_features = len(feature_list[0][1])\n",
    "\n",
    "print data_features.take(1), data_features.count(), number_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'1', 'C11')]\n"
     ]
    }
   ],
   "source": [
    "# Do not create dummy for course, but map the course name to another dictionary\n",
    "\n",
    "course_dictionary = {\n",
    "    'SZagToBU00pLXq691xXO8bju': 'C1', 'AenHr7h6SmttABSZzLDIOOPg': 'C2', 'ATVfuvcgCpCeoJ35eYmFgTKM': 'C3', \n",
    "    'V6HKtzXgLxJKuspYS6oIf6TM': 'C4', 'DKmoqeihogHvweNuFRglX4gC': 'C5', '0amLAQWMHCsmYB0LbJgYyaQh': 'C6', \n",
    "    'm88du8j2zKLOgu0qvfDih64t': 'C7', 'MSJxWgbSMt16zUqycBbrpfm6': 'C8', 'IgPEMpQbX7H5OGTWUvmRmPIO': 'C9', \n",
    "    've31Ekh7zequ7hWZ90igqwDp':'C10', 'ev4oSPQOjeL5QAf5oF72ooYu':'C11', 'QR0XzCHCYDjlY08IzEL9Xoci':'C12', \n",
    "    'K6WjDciFRURqcBkA0O6jIAWI':'C13', '1A29SKaQQCQZabKDaq7Tk8Lo':'C14', '1VZLSdIQsjy9MTgBTQYE4QI2':'C15', \n",
    "    'meLIi3tsNI0i6fexqFOUCz1k':'C16', 'V0ltVkyedZiGeSpNdmwI3AGe':'C17', 'dMOkjBKrDhejoADxQd7zkGiQ':'C18', \n",
    "    'yu4cZXxuoaajHNIVZxVenLrR':'C19', 'ccVlU1h9PciWvbZjm1DvqjcL':'C20', '4y2oIXcbPEgXpmqElRNICJnU':'C21', \n",
    "    'OaSRaqvAQY9msGG5HOGGCxe8':'C22', 'il7yokxrygymsYcIeKmHztAS':'C23', 'oUmjmhtu3lTMYfUMe7EjTl5O':'C24', \n",
    "    'Z6ta6CAHyTVLze24SBoSufZk':'C25', 'ElWfvzJwWRCO8dwcFErfTWeJ':'C26', 'djBWhu0JoDsrQ2a6Kzg6B4E2':'C27', \n",
    "    '72ea9t9dsOcCDKJcFDdyFw7x':'C28', 'TRrZ9gGs6MrmfBbpM1B9hzbn':'C29', '4977pk0QkRxSMP841C5ZDSGk':'C30', \n",
    "    'nknoexvjeVdLxiDT0VODT9CV':'C31', '22sM5fCFEc6YqlavAZ5fQ9rs':'C32', '9PacRT9Ksez8qfqnr0B4rUZr':'C33', \n",
    "    'ctO4FKH4WwG74Jm4loSZHZT7':'C34', 'Cfz86nNsKVtSr4UmAlqazDyb':'C35', 'kxtZS4d61l2cEp0BZ3e6HzOH':'C36', \n",
    "    'BRYxq4ELEbiuSpUBWNPvla8e':'C37', 'T2KaDT5KTn3hGQbrO7nLrErU':'C38', 'WckhTTKCVMWHr8RQsf97UQnr':'C39'\n",
    "}\n",
    "\n",
    "course = enrollment_data.map(lambda e: (e[0], course_dictionary[e[2]]) )\n",
    "course_head = ['course']\n",
    "\n",
    "print course.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" not used temp \"\"\"\n",
    "\n",
    "##### check how many courses in the dataset --> create dummy variables for each course\n",
    "\n",
    "enrollment_data_check = enrollment_data.map(lambda e: (e[2], 1)).reduceByKey(lambda x,y: x+y)\n",
    "enrollment_data_list = enrollment_data_check.map(lambda e: e[0] ).collect()\n",
    "index_list = [i for i in xrange(39)]\n",
    "course_dict = dict(zip(enrollment_data_list,index_list))\n",
    "dummylist = list(np.zeros(39))\n",
    "\n",
    "def course_dummy(e):\n",
    "    new_list = [int(x) for x in dummylist]\n",
    "    new_list[course_dict[e[1]]] = 1\n",
    "    return ( e[0], new_list )\n",
    "\n",
    "course_dummy = enrollment_data.map(lambda e: ( e[0], e[2]) ).map(course_dummy)\n",
    "course_dummy_head = course_dict.keys()\n",
    "\n",
    "print course_head\n",
    "print course.take(1)\n",
    "\n",
    "\n",
    "##### join course with dummy variables to form whole dataset\n",
    "\n",
    "data_whole = data_features.join(course).map(break_list)\n",
    "data_head = data_day_head + data_timeLag_head + data_freq_head + data_dateTime_head + data_pairPattern_head + \\\n",
    "            data_timePair_mean_head + data_timePair_var_head + data_timePair_min_head + data_timePair_max_head + \\\n",
    "            course_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'103941', [3, 24060.0, 69, 7, 30, 25, 8, 10, 2, 1050, 21, 42, 285, 507, 413, 319, 55, 17, 182, 148, 17, 1, 21, 77, 7, 30, 12, 56, 0, 7, 7, 14, 0, 174, 185, 112, 24, 72, 90, 42, 24, 0, 116, 31, 17, 56, 59, 10, 2, 28, 8, 8, 16, 0, 22, 2, 2, 1, 7145.0, 876.0, 2545.0, 55453.0, 628.0, 0.0, 5533.0, 'C33'])] 120542\n",
      "['number of days', 'working time interval', 'access', 'discussion', 'navigate', 'page_close', 'problem', 'video', 'wiki', 'AA', 'AB', 'BA', 'AC', 'CA', 'AD', 'DA', 'AE', 'EA', 'AF', 'FA', 'AG', 'GA', 'BB', 'BC', 'CB', 'BD', 'DB', 'BE', 'EB', 'BF', 'FB', 'BG', 'GB', 'CC', 'CD', 'DC', 'CE', 'EC', 'CF', 'FC', 'CG', 'GC', 'DD', 'DE', 'ED', 'DF', 'FD', 'DG', 'GD', 'EE', 'EF', 'FE', 'EG', 'GE', 'FF', 'FG', 'GF', 'GG', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'course']\n"
     ]
    }
   ],
   "source": [
    "# join course without dummy to form another whole dataset\n",
    "\n",
    "def bre_course(e):\n",
    "    alist = []\n",
    "    for i in xrange(len(e[1][0])):\n",
    "        alist.append(e[1][0][i])\n",
    "    alist.append(e[1][1])\n",
    "    return (e[0], alist)\n",
    "\n",
    "data_whole = data_features.join(course).map(bre_course)\n",
    "data_head = data_day_head + data_timeLag_head + data_freq_head + data_pairPattern_head + \\\n",
    "data_activity_lag_head + course_head\n",
    "\n",
    "print data_whole.take(1), data_whole.count()\n",
    "print data_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'103941', [0.10642912981595662, 3.0740990183713741, 0.79350150026024258, 0.042495389606601174, 1.6668819227598972, 0.70655590433194115, -0.071569137197939184, 0.22919857345051339, 0.31907527446323569, 0.022536646502868431, -0.081162844842727996, -0.018994033091972236, 1.0685482233251613, 1.6508829646527452, 0.52458591462079762, 0.447315718184916, -0.14271973772716573, -0.1490579466063012, 0.16798880557577572, 0.14346997906146014, 0.13918819133375129, -0.096683045452527733, -0.018881577301375466, 0.22336202672639066, -0.057770024816175089, 0.036706344018757504, -0.057791744398984962, 0.090563170322711259, -0.11637023981870402, -0.038314095524840892, -0.080088068663310599, 0.040654877336135264, -0.025538854610072503, 1.1845698046133475, 1.5674027035629916, 1.1158914816535703, -0.059796528663507403, 0.39047827713181332, 1.1435862779128594, 0.5507285101214453, 0.51371359630982516, -0.075725088831687479, 0.2681908425557864, -0.09892528772953417, -0.12850489766144638, 0.10708545238645485, 0.13678207637706144, 0.25509412019184513, -0.028929899948083992, -0.097350102451027112, -0.11674811301097233, -0.13716072232141358, 0.35741995853421948, -0.070739376290392855, -0.066018212923755815, 0.0035843602882952908, 0.035748782220148254, -0.015822441539389481, 0.14533824276382215, -0.019460352178214343, 0.035952975448955975, 2.6304589865461305, 0.042000005456244376, -0.27688977382954866, 3.1948755180807655, 'C33'])]\n"
     ]
    }
   ],
   "source": [
    "# apply standard scaler to the whole dataset and return standardized features.\n",
    "\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "feature = data_features.map(lambda e: e[1])\n",
    "label = data_features.map(lambda e: e[0])\n",
    "\n",
    "scaler = StandardScaler(withMean=True, withStd=True).fit(feature)\n",
    "data_scaler = label.zip(scaler.transform(feature))\n",
    "\n",
    "output_data_scaler = data_scaler.map(lambda e: [ e[0], [x for x in e[1]] ] ).join(course).map(bre_course)\n",
    "print output_data_scaler.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 143.0, 7, 6, 6, 4, 0, 2, 2, 21, 20, 22, 12, 30, 20, 8, 0, 0, 9, 5, 6, 8, 15, 11, 25, 15, 9, 0, 0, 6, 6, 4, 8, 15, 19, 5, 0, 0, 9, 3, 8, 4, 6, 0, 0, 5, 3, 2, 6, 0, 0, 0, 0, 0, 1, 1, 3, 1, 18.0, 73.0, 23.0, 26.0, 0, 1.0, 2.0, 'C6', 1.0]] 72325\n",
      "[[1, 0.0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'C24']] 48217\n",
      "[[-0.53600959755703792, -0.57200746056132679, -0.34580304350736379, 0.016163473735632355, -0.1829045663608283, -0.30070666135160101, -0.30407853803784241, -0.31191525238144718, 0.31907527446323569, -0.017899963960570756, -0.08254167959590801, -0.049394170342278693, -0.19227203075269006, -0.20193389436096226, -0.24736755587229362, -0.21687931953533707, -0.19280503604449134, -0.16918647665003184, -0.21036326108824013, -0.1920945069170861, -0.024795371555005663, 0.027906442333069555, -0.020256271976629228, -0.021625444581344452, -0.00074132505078124123, -0.021765475272879913, -0.068655962891513947, -0.084093150836081501, -0.11637023981870402, -0.046018343288704196, -0.086943134830281488, -0.0027801717645399003, 0.020472326641835677, -0.085118117959001732, -0.14659600491470717, -0.17642584474684211, -0.23163652099069451, -0.1836631652999077, -0.17696763129658727, -0.17293311988124943, 0.091921126099448039, 0.034365199121821975, -0.25139868545911875, -0.2062966091834828, -0.1976945000569425, -0.23108665577550558, -0.24326771301744091, -0.035170252514559019, 0.13841545423784082, -0.11227649796209523, -0.15098571850037606, -0.16108062889086325, -0.093444964485118867, -0.070739376290392855, -0.17566251005418282, -0.046902430769392441, 0.10630514253748118, -0.015822441539389481, -0.29871824760857646, -0.074304160998771268, -0.2079025470327926, -0.28153086261351828, -0.16089284649282606, -0.27681315659619377, -0.062482898919941202, 'C6', 1.0]] 72325\n",
      "[[-0.53600959755703792, -0.59380757037808374, -0.47443420167467421, -0.14182802149018056, -0.56827675159431279, -0.49256619767227572, -0.30407853803784241, -0.44719370883943732, -0.19178590266670256, -0.01872520090880421, -0.11011837465950845, -0.082834321317615797, -0.24769270126160661, -0.31846325656308222, -0.28665271976507178, -0.23396472243489344, -0.19280503604449134, -0.16918647665003184, -0.23004631657943173, -0.20382753090234995, -0.11424095131250946, -0.11448154370761307, -0.023693008664763641, -0.062456689799300309, -0.079947852502717132, -0.080237294564517336, -0.10124861836910087, -0.084093150836081501, -0.11637023981870402, -0.092243829871884023, -0.1280735318321069, -0.020154191404809969, -0.025538854610072503, -0.20489999744695919, -0.34277657998143174, -0.23681450485901093, -0.23163652099069451, -0.1836631652999077, -0.32369584343097024, -0.22859939911222596, -0.11897510900574053, -0.075725088831687479, -0.27973993244174999, -0.2062966091834828, -0.1976945000569425, -0.26424078402667817, -0.26362752316357496, -0.10773634569116006, -0.1126025770410464, -0.11227649796209523, -0.15098571850037606, -0.16108062889086325, -0.093444964485118867, -0.070739376290392855, -0.18088366706039363, -0.09738922182708018, -0.10536393841451759, -0.022507833363263119, -0.29983975972120491, -0.079289961800640074, -0.21012644751537793, -0.28289683472238492, -0.16089284649282606, -0.27688977382954866, -0.063660754069823938, 'C24']] 48217\n"
     ]
    }
   ],
   "source": [
    "# create training data and testing data --> training data with label, testing without label\n",
    "# one combination of scaler, one without scaler\n",
    "\n",
    "def merge_train(e):\n",
    "    alist = []\n",
    "    for value in e[0]:\n",
    "        alist.append(value)\n",
    "    alist.append(e[1])\n",
    "    return alist\n",
    "\n",
    "training_no_scale = data_whole.join(train_data).map(lambda e: e[1]).map(merge_train)\n",
    "testing_no_scale = data_whole.join(submission_data).map(lambda e: e[1][0])\n",
    "\n",
    "training_scale = output_data_scaler.join(train_data).map(lambda e: e[1]).map(merge_train)\n",
    "testing_scale = output_data_scaler.join(submission_data).map(lambda e: e[1][0])\n",
    "\n",
    "print training_no_scale.take(1), training_no_scale.count()\n",
    "print testing_no_scale.take(1), testing_no_scale.count()\n",
    "\n",
    "print training_scale.take(1), training_scale.count()\n",
    "print testing_scale.take(1), testing_scale.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare for outputing the training and testing dataset\n",
    "\n",
    "train_list = training_no_scale.collect()\n",
    "test_list = testing_no_scale.collect()\n",
    "\n",
    "train_scale_list = training_scale.collect()\n",
    "test_scale_list = testing_scale.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output head\n",
    "\n",
    "with open('round2_head.csv','wb') as out:\n",
    "    csv_out2=csv.writer(out)\n",
    "    csv_out2.writerow(data_head)\n",
    "    for row in data_head:\n",
    "        csv_out2.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output no scale training data as csv file  \n",
    "\n",
    "data_heading = data_head + ['target']\n",
    "\n",
    "with open('round2_train_no_scale.csv','wb') as out:\n",
    "    csv_out2=csv.writer(out)\n",
    "    csv_out2.writerow(data_heading)\n",
    "    for row in train_list:\n",
    "        csv_out2.writerow(row)\n",
    "\n",
    "\n",
    "# output no scale testing data as csv file \n",
    "\n",
    "with open('round2_test_no_scale.csv','wb') as out:\n",
    "    csv_out2=csv.writer(out)\n",
    "    csv_out2.writerow(data_head)\n",
    "    for row in test_list:\n",
    "        csv_out2.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output with scale training data as csv file \n",
    "\n",
    "data_heading = data_head + ['target']\n",
    "\n",
    "with open('round2_train_data.csv','wb') as out:\n",
    "    csv_out2=csv.writer(out)\n",
    "    csv_out2.writerow(data_heading)\n",
    "    for row in train_scale_list:\n",
    "        csv_out2.writerow(row)\n",
    "\n",
    "        \n",
    "# output with scale testing data as csv file\n",
    "\n",
    "with open('round2_test_data.csv','wb') as out:\n",
    "    csv_out2=csv.writer(out)\n",
    "    csv_out2.writerow(data_head)\n",
    "    for row in test_scale_list:\n",
    "        csv_out2.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
